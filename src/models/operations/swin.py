from typing import Callable, Optional, Tuple
import itertools
import torch
import torch.nn as nn
import math
import warnings


def trunc_normal_(
    tensor: torch.Tensor,
    mean: float = 0.0,
    std: float = 1.0,
    a: float = -2.0,
    b: float = 2.0,
) -> torch.Tensor:
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    # Values are generated by using a truncated uniform distribution and
    # then using the inverse CDF for the normal distribution.
    # Get upper and lower cdf values
    lower = norm_cdf((a - mean) / std)
    upper = norm_cdf((b - mean) / std)

    # Uniformly fill tensor with values from [l, u], then translate to
    # [2l-1, 2u-1].
    tensor.uniform_(2 * lower - 1, 2 * upper - 1)

    # Use inverse cdf transform for normal distribution to get truncated
    # standard normal
    tensor.erfinv_()

    # Transform to proper mean, std
    tensor.mul_(std * math.sqrt(2.))
    tensor.add_(mean)

    # Clamp to ensure it's in the proper range
    tensor.clamp_(min=a, max=b)
    return tensor


class WindowAttention(nn.Module):
    r""" Window based multi-head self attention (W-MSA) module with relative position bias.
    It supports both of shifted and non-shifted window.
    Args:
        dim (int): Number of input channels.
        window_size (tuple[int]): The height and width of the window.
        num_heads (int): Number of attention heads.
        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True
        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0
        proj_drop (float, optional): Dropout ratio of output. Default: 0.0
    """

    def __init__(
        self,
        channels: int,
        window_size: Tuple[int, int],
        num_heads: int,
        qkv_bias: bool = True,
        attn_drop: float = 0.0,
        proj_drop: float = 0.0,
    ) -> None:

        super().__init__()
        self.channels = channels
        self.window_size = window_size  # Wh, Ww
        self.num_heads = num_heads
        self.scale = (channels // num_heads) ** -0.5

        self.qkv = nn.Linear(channels, channels * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(channels, channels)
        self.proj_drop = nn.Dropout(proj_drop)
        self.softmax = nn.Softmax(dim=-1)

        # define a parameter table of relative position bias
        relative_position_bias_table = torch.zeros(
            (2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)  # 2*Wh-1 * 2*Ww-1, nH
        trunc_normal_(relative_position_bias_table, std=.02)

        # get pair-wise relative position index for each token inside the window
        coords_h = [i for i in range(window_size[0]) for _ in range(window_size[1])]
        coords_w = [i for _ in range(window_size[0]) for i in range(window_size[1])]
        coords_flatten = torch.Tensor([coords_h, coords_w]).long()
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww
        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2
        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0
        relative_coords[:, :, 1] += self.window_size[1] - 1
        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1
        relative_position_index = relative_coords.sum(-1).view(-1)  # Wh*Ww, Wh*Ww

        # set a parameter of relative position bias
        relative_position_bias = relative_position_bias_table[relative_position_index].view(
            window_size[0] * window_size[1], window_size[0] * window_size[1], -1)  # Wh*Ww,Wh*Ww,nH
        relative_position_bias = relative_position_bias.permute(2, 0, 1)  # nH, Wh*Ww, Wh*Ww
        relative_position_bias = relative_position_bias.unsqueeze(0).contiguous()
        self.relative_position_bias = nn.Parameter(relative_position_bias.detach())

    def forward(
        self,
        x: torch.Tensor,
        mask: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        """
        Args:
            x: input features with shape of (num_windows*B, N, C)
            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None
        """
        qkv = self.qkv(x)
        qkv = qkv.reshape(
            x.shape[0], x.shape[1], 3, self.num_heads, self.channels // self.num_heads)
        qkv = qkv.permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]

        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn + self.relative_position_bias

        if mask is not None:
            attn = attn.reshape(-1, mask.shape[0], self.num_heads, x.shape[1], x.shape[1])
            attn = attn + mask.unsqueeze(1).unsqueeze(0)
            attn = attn.reshape(-1, self.num_heads, x.shape[1], x.shape[1])
            attn = self.softmax(attn)
        else:
            attn = self.softmax(attn)

        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2)
        x = x.reshape(x.shape[0], x.shape[1], self.channels)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x


class SwinOperation(nn.Module):
    '''
    Operation class for Swin Transformers.
    https://arxiv.org/abs/2103.14030
    '''

    def __init__(
        self,
        in_channels: int,  # in_channels might be changed by PatchMerging.
        out_channels: int,
        feature_size: int,
        window_size: int,
        shift_size: int,
        attn_heads: int,
        mlp_ratio: float,
        normalization: Callable[..., nn.Module],
        activation: Callable[..., nn.Module],
        operation_type: str,
        dropout_prob: float,
        **kwargs,
    ) -> None:
        super().__init__()
        self.attn_op = (operation_type.lower() == 'attn')
        self.window_size = window_size
        self.shift_size = shift_size

        if self.attn_op:
            self.attn_norm = normalization(out_channels)
            self.attn_attn = WindowAttention(
                out_channels, (window_size, window_size), attn_heads)
        else:
            mid_channels = int(out_channels * mlp_ratio)
            self.mlp_norm = normalization(out_channels)
            self.mlp_fc1 = nn.Linear(out_channels, mid_channels)
            self.mlp_drop1 = nn.Dropout(p=dropout_prob, inplace=True)
            self.mlp_act = activation(inplace=True)
            self.mlp_fc2 = nn.Linear(mid_channels, out_channels)
            self.mlp_drop2 = nn.Dropout(p=dropout_prob, inplace=True)

        if shift_size > 0:
            # calculate attention mask for SW-MSA
            attn_mask = torch.zeros((feature_size, feature_size))
            h_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            w_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))

            for i, (h, w) in enumerate(itertools.product(h_slices, w_slices)):
                attn_mask[h, w] = i

            attn_mask = attn_mask.view(
                feature_size // window_size, window_size,
                feature_size // window_size, window_size)
            attn_mask = attn_mask.permute(0, 2, 1, 3).contiguous()
            attn_mask = attn_mask.view(-1, window_size * window_size)

            attn_mask = attn_mask.unsqueeze(1) - attn_mask.unsqueeze(2)
            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0))
            self.register_buffer("attn_mask", attn_mask, persistent=False)
        else:
            self.register_buffer("attn_mask", None, persistent=False)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if self.attn_op:  # attention
            return self.forward_attn(x)
        else:  # mlp
            return self.forward_mlp(x)

    def forward_attn(self, x: torch.Tensor) -> torch.Tensor:
        size_b, size_c, size_h, size_w = x.shape
        x = x.permute(0, 2, 3, 1).reshape(size_b, -1, size_c)

        # normalize
        x = self.attn_norm(x)
        x = x.reshape(size_b, size_h, size_w, size_c)

        # cyclic shift
        if self.shift_size > 0:
            x = torch.cat([x[:, self.shift_size:], x[:, :self.shift_size]], dim=1)
            x = torch.cat([x[:, :, self.shift_size:], x[:, :, :self.shift_size]], dim=2)

        # partition windows
        # nW*B, window_size, window_size, C
        x = x.reshape(*x.shape[:1], -1, self.window_size, *x.shape[-2:])
        x = x.reshape(*x.shape[:3], -1, self.window_size, *x.shape[-1:])
        x = x.permute(0, 1, 3, 2, 4, 5)
        patch_h, patch_w = x.shape[1:3]
        x = x.reshape(-1, self.window_size, self.window_size, size_c)
        # nW*B, window_size*window_size, C
        x = x.reshape(-1, self.window_size * self.window_size, size_c)

        # W-MSA/SW-MSA
        # nW*B, window_size*window_size, C
        x = self.attn_attn(x, mask=self.attn_mask)

        # merge windows
        x = x.reshape(-1, self.window_size, self.window_size, size_c)
        # B H' W' C
        x = x.reshape(size_b, patch_h, patch_w, self.window_size, self.window_size, -1)
        x = x.permute(0, 1, 3, 2, 4, 5).reshape(size_b, size_h, size_w, -1)

        # reverse cyclic shift
        if self.shift_size > 0:
            x = torch.cat([x[:, -self.shift_size:], x[:, :-self.shift_size]], dim=1)
            x = torch.cat([x[:, :, -self.shift_size:], x[:, :, :-self.shift_size]], dim=2)

        x = x.permute(0, 3, 1, 2)

        return x

    def forward_mlp(self, x: torch.Tensor) -> torch.Tensor:
        size = x.shape[:]
        x = x.permute(0, 2, 3, 1)
        x = x.reshape(size[0], -1, size[1])
        x = self.mlp_norm(x)
        x = self.mlp_fc1(x)
        x = self.mlp_drop1(x)
        x = self.mlp_act(x)
        x = self.mlp_fc2(x)
        x = self.mlp_drop2(x)
        x = x.reshape(size[0], size[2], size[3], size[1])
        x = x.permute(0, 3, 1, 2)

        return x
